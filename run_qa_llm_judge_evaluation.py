"""
Run QA evaluation using LLM-as-Judge methodology and generate comparison reports.

This script evaluates Question-Answer benchmark results using LLM-as-Judge methodology:
1. Loads QA results from CSV files (one per model) generated by QA benchmark scripts
2. Evaluates answer quality using LLM judge (GPT-4o via OpenRouter)
3. Generates performance comparison visualizations
4. Creates detailed reports with model rankings and breakdowns

The LLM-as-Judge approach uses a powerful model (default: GPT-4o) to score the quality
of answers from other models, providing more nuanced evaluation than simple metrics.
"""
import os
import argparse
from pathlib import Path
from typing import Dict, List
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from collections import defaultdict

from src.benchmark.qa_evaluator import QAEvaluator

# API key from environment variables
from src.utils.config import require_api_key
OPEN_ROUTER_API_KEY = require_api_key('OPEN_ROUTER_API_KEY', 'OpenRouter')

# Official model names mapping
OFFICIAL_MODEL_NAMES = {
    'claude_opus_4_5': 'Claude Opus 4.5',
    'claude_opus_45': 'Claude Opus 4.5',
    'claude_sonnet_4_5': 'Claude Sonnet 4.5',
    'claude_sonnet_45': 'Claude Sonnet 4.5',
    'gemini_3': 'Gemini 3',
    'gemini_3_pro_preview': 'Gemini 3',
    'openai_gpt_4_vision': 'GPT-4o',
    'gpt_4o': 'GPT-4o',
    'grok_41_fast': 'Grok 4.1 Fast',
    'qwen3_vl_8b_instruct': 'Qwen3 VL 8B Instruct',
    'glm_45v': 'GLM-4.5V',
    'glm_46v': 'GLM-4.6V',
    'glm_4_6v': 'GLM-4.6V',
    'mistral_large_2512': 'Mistral Large 3',
    'openai_gpt_51': 'OpenAI GPT-5.1',
    'openai_gpt_52': 'OpenAI GPT-5.2',
    'amazon_nova_2_lite_v1': 'Amazon Nova 2 Lite v1',
    'cohere_command_a_vision': 'Cohere Command A Vision',
    'nvidia_nemotron_nano_12b_v2_vl': 'Nvidia Nemotron Nano 12B',
}

# Standard QA type names mapping
QA_TYPE_NAMES = {
    'ocr_qa': 'Text Extraction (OCR)',
    'spatial_qa': 'Spatial Reasoning',
    'counting_qa': 'Instance Counting',
    'comparison_qa': 'Comparative Reasoning',
}

# Model display order for visualizations (keywords to match model names)
# MMLMs on the left, VLMs on the right
MODEL_DISPLAY_ORDER = [
    'gemini 3',           # 1. gemini 3
    'openai gpt-5.2',     # 2. openai 5.2
    'openai gpt 5.2',
    'opus 4.5',           # 3. opus 4.5
    'claude opus',
    'mistral large',      # 4. mistral
    'amazon',             # 5. Amazon Nova
    'grok',               # 6. grok
    'qwen3 vl 8b instruct',  # 7. qwen 8b instruct (VLM)
    'qwen3-vl 8b',
    'glm-4.6v',           # 8. glm_46v (VLM)
    'glm 46v',
    'cohere',             # 9. cohere (VLM)
    'nvidia',             # 10. nvidia (VLM)
]

# Open-source models (keywords to match model names)
# Models not in this list are considered proprietary/closed-source
OPEN_SOURCE_MODELS = [
    'qwen',       # Qwen models (Alibaba)
    'nemotron',   # NVIDIA Nemotron
    'nvidia',     # NVIDIA models
    'glm',        # GLM models (Zhipu AI)
    'mistral',    # Mistral models (Mistral AI)
]

# Set style for better visualizations
sns.set_style("whitegrid")
plt.rcParams['figure.figsize'] = (12, 8)


def load_qa_results(csv_path: str) -> pd.DataFrame:
    """Load QA results from CSV file."""
    return pd.read_csv(csv_path)


def evaluate_model_results(csv_path: str, evaluator: QAEvaluator) -> Dict:
    """
    Evaluate a single model's results.
    
    Args:
        csv_path: Path to the model's QA results CSV
        evaluator: QAEvaluator instance
        
    Returns:
        Dictionary with evaluation results
    """
    print(f"\nEvaluating: {csv_path}")
    evaluation = evaluator.evaluate_csv(csv_path)
    
    # Extract model name from filename
    raw_model_name = Path(csv_path).stem.replace('_qa_results', '').replace('qa_results', '').strip('_')
    # Convert to official name if available
    model_name = OFFICIAL_MODEL_NAMES.get(raw_model_name, raw_model_name.replace('_', ' ').title())
    
    return {
        'model_name': model_name,
        'raw_model_name': raw_model_name,
        'csv_path': csv_path,
        **evaluation
    }


def sort_evaluations_by_display_order(evaluations: List[Dict]) -> List[Dict]:
    """
    Sort evaluations according to MODEL_DISPLAY_ORDER.

    Args:
        evaluations: List of evaluation dictionaries

    Returns:
        Sorted list of evaluations
    """
    def get_sort_key(eval_data):
        model_name = eval_data.get('model_name', '').lower()
        raw_model_name = eval_data.get('raw_model_name', '').lower()
        combined_name = f"{model_name} {raw_model_name}".lower()

        # Find the index in MODEL_DISPLAY_ORDER
        for i, order_keyword in enumerate(MODEL_DISPLAY_ORDER):
            if order_keyword.lower() in combined_name:
                return i
        # If not found, put at the end
        return len(MODEL_DISPLAY_ORDER) + 1000

    return sorted(evaluations, key=get_sort_key)


def sort_evaluations_by_score_descending(evaluations: List[Dict]) -> List[Dict]:
    """
    Sort evaluations by overall score in descending order.

    Args:
        evaluations: List of evaluation dictionaries

    Returns:
        Sorted list of evaluations (highest score first)
    """
    return sorted(evaluations, key=lambda x: x['summary']['mean_overall_score'], reverse=True)


def is_open_source_model(model_name: str, raw_model_name: str = '') -> bool:
    """
    Check if a model is open-source based on its name.

    Args:
        model_name: Display name of the model
        raw_model_name: Raw model name from filename

    Returns:
        True if model is open-source, False if proprietary
    """
    combined_name = f"{model_name} {raw_model_name}".lower()

    for keyword in OPEN_SOURCE_MODELS:
        if keyword.lower() in combined_name:
            return True
    return False


def create_comparison_visualizations(evaluations: List[Dict], output_dir: str):
    """
    Create visualizations comparing model performance.
    
    Args:
        evaluations: List of evaluation dictionaries
        output_dir: Directory to save visualizations
    """
    os.makedirs(output_dir, exist_ok=True)
    
    # Filter out older model versions and auxiliary files from visualizations
    # Only keep latest models from each vendor
    filtered_evaluations = [
        e for e in evaluations
        if 'qwen3_vl_8b_thinking' not in e.get('raw_model_name', '').lower()
        and 'Qwen3 VL 8B Thinking' not in e.get('model_name', '')
        and 'openai_gpt_51' not in e.get('raw_model_name', '').lower()
        and 'OpenAI GPT-5.1' not in e.get('model_name', '')
        and 'openai_gpt_4_vision' not in e.get('raw_model_name', '').lower()
        and 'gpt_4o' not in e.get('raw_model_name', '').lower()
        and 'GPT-4o' not in e.get('model_name', '')
        and 'glm_45v' not in e.get('raw_model_name', '').lower()
        and 'GLM-4.5V' not in e.get('model_name', '')
        and 'complete' not in e.get('raw_model_name', '').lower()
        and 'detailed' not in e.get('raw_model_name', '').lower()
        and 'Complete' not in e.get('model_name', '')
        and 'Detailed' not in e.get('model_name', '')
    ]
    
    # Sort evaluations by score in descending order
    filtered_evaluations = sort_evaluations_by_score_descending(filtered_evaluations)

    # Prepare data for plotting
    model_names = [e['model_name'] for e in filtered_evaluations]
    overall_scores = [e['summary']['mean_overall_score'] for e in filtered_evaluations]

    # Determine colors based on model type (proprietary vs open-source)
    COLOR_PROPRIETARY = '#4472C4'  # Blue for proprietary/closed-source
    COLOR_OPEN_SOURCE = '#70AD47'  # Green for open-source
    bar_colors = [
        COLOR_OPEN_SOURCE if is_open_source_model(e['model_name'], e.get('raw_model_name', ''))
        else COLOR_PROPRIETARY
        for e in filtered_evaluations
    ]

    # 1. Overall Performance Comparison
    fig, ax = plt.subplots(figsize=(12, 6))
    x = range(len(model_names))
    width = 0.6
    bars = ax.bar(x, overall_scores, width, color=bar_colors)
    ax.set_xlabel('Model', fontsize=12, fontweight='bold')
    ax.set_ylabel('Accuracy Score', fontsize=12, fontweight='bold')
    ax.set_title('Model Performance Comparison on document QA', fontsize=14, fontweight='bold')
    ax.set_xticks(x)
    ax.set_xticklabels(model_names, rotation=45, ha='right')
    ax.set_ylim([0, 1])
    ax.grid(axis='y', alpha=0.3)

    # Add legend for model types
    from matplotlib.patches import Patch
    legend_elements = [
        Patch(facecolor=COLOR_PROPRIETARY, label='Proprietary'),
        Patch(facecolor=COLOR_OPEN_SOURCE, label='Open-Source')
    ]
    ax.legend(handles=legend_elements, loc='upper right')

    # Add value labels on bars
    for i, (bar, score) in enumerate(zip(bars, overall_scores)):
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height,
                f'{score:.3f}',
                ha='center', va='bottom', fontsize=10)

    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'overall_comparison.svg'), format='svg', bbox_inches='tight')
    plt.savefig(os.path.join(output_dir, 'overall_comparison.png'), format='png', dpi=300, bbox_inches='tight')
    plt.close()

    # 2. QA Type Performance
    # Define specific order and colors for QA types
    qa_type_order = ['spatial_qa', 'ocr_qa', 'comparison_qa', 'counting_qa']
    qa_type_colors = {
        'spatial_qa': '#E8956A',      # Orange
        'ocr_qa': '#6A9BE8',           # Blue
        'comparison_qa': '#6AE89B',    # Green
        'counting_qa': '#B86AE8',      # Purple
    }

    # Collect all available QA types
    all_qa_types = set()
    for eval_data in filtered_evaluations:
        all_qa_types.update(eval_data['summary'].get('qa_type_breakdown', {}).keys())

    # Filter and order QA types
    ordered_qa_types = [qt for qt in qa_type_order if qt in all_qa_types]

    if ordered_qa_types:
        qa_type_data = {}
        for qa_type in ordered_qa_types:
            qa_type_data[qa_type] = []
            for eval_data in filtered_evaluations:
                qa_breakdown = eval_data['summary'].get('qa_type_breakdown', {})
                if qa_type in qa_breakdown:
                    qa_type_data[qa_type].append(qa_breakdown[qa_type]['mean_score'])
                else:
                    qa_type_data[qa_type].append(0.0)

        fig, ax = plt.subplots(figsize=(12, 6))
        x = range(len(model_names))
        width = 0.8 / len(ordered_qa_types)

        for i, qa_type in enumerate(ordered_qa_types):
            scores = qa_type_data[qa_type]
            offset = (i - len(ordered_qa_types)/2) * width + width/2
            # Use standard QA type name
            qa_type_label = QA_TYPE_NAMES.get(qa_type, qa_type.replace('_', ' ').title())
            color = qa_type_colors.get(qa_type, '#888888')
            ax.bar([xi + offset for xi in x], scores, width, label=qa_type_label, color=color)

        ax.set_xlabel('Model', fontsize=12, fontweight='bold')
        ax.set_ylabel('Mean Score', fontsize=12, fontweight='bold')
        ax.set_title('Model Performance by QA Type', fontsize=14, fontweight='bold')
        ax.set_xticks(x)
        ax.set_xticklabels(model_names, rotation=45, ha='right')
        ax.set_ylim([0, 1])
        ax.legend(loc='upper right')
        ax.grid(axis='y', alpha=0.3)

        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, 'qa_type_breakdown.svg'), format='svg', bbox_inches='tight')
        plt.savefig(os.path.join(output_dir, 'qa_type_breakdown.png'), format='png', dpi=300, bbox_inches='tight')
        plt.close()


def generate_detailed_report(evaluations: List[Dict], output_dir: str):
    """
    Generate a detailed text report with model rankings and analysis.
    
    Args:
        evaluations: List of evaluation dictionaries
        output_dir: Directory to save the report
    """
    os.makedirs(output_dir, exist_ok=True)
    report_path = os.path.join(output_dir, 'evaluation_report.txt')
    
    # Sort by overall score
    sorted_evaluations = sorted(evaluations, key=lambda x: x['summary']['mean_overall_score'], reverse=True)
    
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("="*80 + "\n")
        f.write("QA BENCHMARK EVALUATION REPORT\n")
        f.write("="*80 + "\n\n")
        
        # Overall Rankings
        f.write("MODEL RANKINGS (by Overall Score)\n")
        f.write("-"*80 + "\n")
        for rank, eval_data in enumerate(sorted_evaluations, 1):
            model_name = eval_data['model_name']
            overall = eval_data['summary']['mean_overall_score']
            total = eval_data['summary']['total_questions']
            f.write(f"{rank}. {model_name:30s} | Overall: {overall:.4f} | Questions: {total}\n")
        f.write("\n")
        
        # Detailed Metrics per Model
        f.write("="*80 + "\n")
        f.write("DETAILED METRICS BY MODEL\n")
        f.write("="*80 + "\n\n")
        
        for eval_data in sorted_evaluations:
            model_name = eval_data['model_name']
            summary = eval_data['summary']
            
            f.write(f"\n{model_name}\n")
            f.write("-"*80 + "\n")
            f.write(f"Total Questions: {summary['total_questions']}\n")
            f.write(f"Accuracy Score: {summary['mean_overall_score']:.4f} (Â±{summary['std_overall_score']:.4f})\n")
            f.write(f"  (Evaluated using LLM-as-Judge: binary scoring)\n")
            
            # Task breakdown
            if summary.get('task_breakdown'):
                f.write("\n  Task Breakdown:\n")
                for task, stats in sorted(summary['task_breakdown'].items()):
                    task_label = task.replace('_', ' ').title()
                    f.write(f"    - {task_label:25s}: {stats['mean_score']:.4f} (n={stats['total']})\n")
            
            # QA type breakdown
            if summary.get('qa_type_breakdown'):
                f.write("\n  QA Type Breakdown:\n")
                for qa_type, stats in sorted(summary['qa_type_breakdown'].items()):
                    qa_type_label = QA_TYPE_NAMES.get(qa_type, qa_type.replace('_', ' ').title())
                    f.write(f"    - {qa_type_label:25s}: {stats['mean_score']:.4f} (n={stats['total']})\n")
        
        f.write("\n" + "="*80 + "\n")
        f.write("END OF REPORT\n")
        f.write("="*80 + "\n")
    
    print(f"Detailed report saved to: {report_path}")


def save_individual_evaluation_csv(evaluations: List[Dict], output_dir: str):
    """Save individual evaluation CSV files for each model."""
    os.makedirs(output_dir, exist_ok=True)
    
    for eval_data in evaluations:
        # Use raw_model_name for filename (filesystem-safe)
        raw_model_name = eval_data.get('raw_model_name', eval_data['model_name'].lower().replace(' ', '_'))
        results = eval_data['results']
        
        if results:
            df = pd.DataFrame(results)
            # Create filename based on raw model name
            csv_filename = f"{raw_model_name}_evaluation_results.csv"
            csv_path = os.path.join(output_dir, csv_filename)
            df.to_csv(csv_path, index=False)
            print(f"Individual evaluation CSV saved: {csv_path}")


def save_complete_evaluation_csv(evaluations: List[Dict], output_dir: str):
    """Save complete evaluation results to a single CSV for visualization."""
    os.makedirs(output_dir, exist_ok=True)
    
    all_results = []
    for eval_data in evaluations:
        model_name = eval_data['model_name']
        for result in eval_data['results']:
            result['model'] = model_name
            all_results.append(result)
    
    if all_results:
        df = pd.DataFrame(all_results)
        csv_path = os.path.join(output_dir, 'complete_evaluation_results.csv')
        df.to_csv(csv_path, index=False)
        print(f"Complete evaluation results CSV saved to: {csv_path}")


def load_existing_evaluation_results(output_dir: str) -> List[Dict]:
    """
    Load existing evaluation results from saved CSV files.
    
    Args:
        output_dir: Directory containing evaluation result CSV files
        
    Returns:
        List of evaluation dictionaries
    """
    evaluations = []
    output_path = Path(output_dir)
    
    # Find all evaluation CSV files (excluding complete and detailed)
    csv_files = [
        f for f in output_path.glob('*_evaluation_results.csv')
        if f.name not in ['complete_evaluation_results.csv', 'detailed_evaluation_results.csv']
    ]
    
    for csv_file in csv_files:
        try:
            df = pd.read_csv(csv_file)
            
            # Extract model name from filename
            filename = csv_file.stem
            raw_model_name = filename.replace('_evaluation_results', '').strip('_')
            
            # Convert to official name if available
            model_name = OFFICIAL_MODEL_NAMES.get(raw_model_name, raw_model_name.replace('_', ' ').title())
            
            # Convert DataFrame to list of dictionaries (results)
            results = df.to_dict('records')
            
            # Reconstruct summary statistics
            if results and 'overall' in results[0]:
                overall_scores = [r['overall'] for r in results if 'overall' in r]
                
                # Task breakdown
                task_stats = defaultdict(lambda: {'scores': [], 'total': 0})
                for result in results:
                    if 'task' in result and 'overall' in result:
                        task = result['task']
                        task_stats[task]['scores'].append(result['overall'])
                        task_stats[task]['total'] += 1
                
                # QA type breakdown
                qa_type_stats = defaultdict(lambda: {'scores': [], 'total': 0})
                for result in results:
                    if 'qa_type' in result and 'overall' in result:
                        qa_type = result['qa_type']
                        qa_type_stats[qa_type]['scores'].append(result['overall'])
                        qa_type_stats[qa_type]['total'] += 1
                
                summary = {
                    'total_questions': len(results),
                    'mean_overall_score': np.mean(overall_scores) if overall_scores else 0.0,
                    'std_overall_score': np.std(overall_scores) if overall_scores else 0.0,
                    'task_breakdown': {
                        task: {
                            'total': stats['total'],
                            'mean_score': np.mean(stats['scores']) if stats['scores'] else 0.0,
                            'std_score': np.std(stats['scores']) if stats['scores'] else 0.0
                        }
                        for task, stats in task_stats.items()
                    },
                    'qa_type_breakdown': {
                        qa_type: {
                            'total': stats['total'],
                            'mean_score': np.mean(stats['scores']) if stats['scores'] else 0.0,
                            'std_score': np.std(stats['scores']) if stats['scores'] else 0.0
                        }
                        for qa_type, stats in qa_type_stats.items()
                    }
                }
            else:
                summary = {'total_questions': 0}
            
            evaluations.append({
                'model_name': model_name,
                'raw_model_name': raw_model_name,
                'csv_path': str(csv_file),
                'results': results,
                'summary': summary
            })
        except Exception as e:
            print(f"Error loading {csv_file.name}: {e}")
    
    return evaluations


def main():
    parser = argparse.ArgumentParser(description='Evaluate QA benchmark results')
    parser.add_argument(
        '--input-dir',
        type=str,
        default='benchmark_result_qa',
        help='Directory containing QA result CSV files (default: benchmark_result_qa)'
    )
    parser.add_argument(
        '--output-dir',
        type=str,
        default='results/qa_llm_judge_results',
        help='Directory to save evaluation results (default: results/qa_llm_judge_results)'
    )
    parser.add_argument(
        '--csv-files',
        type=str,
        nargs='+',
        help='Specific CSV files to evaluate (optional, otherwise finds all in input-dir)'
    )
    parser.add_argument(
        '--judge-model',
        type=str,
        default='openai/gpt-4o',
        help='Model to use as judge (default: openai/gpt-4o)'
    )
    
    args = parser.parse_args()
    
    # Resolve output directory to absolute path
    output_dir = os.path.abspath(args.output_dir)
    os.makedirs(output_dir, exist_ok=True)
    print(f"Output directory: {output_dir}")
    
    # Check if evaluation results already exist
    existing_evaluations = load_existing_evaluation_results(output_dir)
    
    # Find CSV files to evaluate
    if args.csv_files:
        csv_files = [Path(f) for f in args.csv_files]
    else:
        input_dir = Path(args.input_dir)
        csv_files = list(input_dir.glob('*.csv'))
    
    if not csv_files:
        print(f"Error: No CSV files found in {args.input_dir}")
        return
    
    # Get set of already evaluated model names (from raw_model_name)
    evaluated_models = {e.get('raw_model_name', '').lower() for e in existing_evaluations}
    
    # Find CSV files that haven't been evaluated yet
    new_csv_files = []
    for csv_file in csv_files:
        # Extract model name from CSV filename
        raw_model_name = csv_file.stem.replace('_qa_results', '').replace('qa_results', '').strip('_')
        if raw_model_name.lower() not in evaluated_models:
            new_csv_files.append(csv_file)
    
    # Start with existing evaluations
    evaluations = existing_evaluations.copy() if existing_evaluations else []
    
    if existing_evaluations:
        print(f"\nFound {len(existing_evaluations)} existing evaluation result(s) in {output_dir}")
        for eval_data in existing_evaluations:
            print(f"  - {eval_data['model_name']} ({eval_data['summary']['total_questions']} questions)")
    
    # Evaluate new CSV files that don't have evaluation results yet
    if new_csv_files:
        print(f"\nFound {len(new_csv_files)} new CSV file(s) to evaluate:")
        for csv_file in new_csv_files:
            raw_model_name = csv_file.stem.replace('_qa_results', '').replace('qa_results', '').strip('_')
            model_name = OFFICIAL_MODEL_NAMES.get(raw_model_name, raw_model_name.replace('_', ' ').title())
            print(f"  - {model_name} ({csv_file.name})")
        
        print("\nRunning LLM-as-Judge evaluation for new files...")
        
        # Initialize evaluator
        print(f"Using judge model: {args.judge_model}")
        evaluator = QAEvaluator(
            judge_model=args.judge_model,
            open_router_api_key=OPEN_ROUTER_API_KEY
        )
        
        # Evaluate each new model
        new_evaluations = []
        for csv_file in new_csv_files:
            try:
                eval_data = evaluate_model_results(str(csv_file), evaluator)
                new_evaluations.append(eval_data)
            except Exception as e:
                print(f"Error evaluating {csv_file}: {e}")
        
        if new_evaluations:
            # Save individual evaluation results for new models
            print("\nSaving individual evaluation results for new models...")
            save_individual_evaluation_csv(new_evaluations, output_dir)
            
            # Add new evaluations to the list
            evaluations.extend(new_evaluations)
            
            # Regenerate complete evaluation results CSV with all models
            print("Updating complete evaluation results...")
            save_complete_evaluation_csv(evaluations, output_dir)
    elif not existing_evaluations:
        # No existing results and no new files (shouldn't happen, but handle it)
        print("\nNo existing evaluation results found. Running LLM-as-Judge evaluation...")
        print(f"Found {len(csv_files)} CSV file(s) to evaluate")
        
        # Initialize evaluator
        print(f"Using judge model: {args.judge_model}")
        evaluator = QAEvaluator(
            judge_model=args.judge_model,
            open_router_api_key=OPEN_ROUTER_API_KEY
        )
        
        # Evaluate each model
        for csv_file in csv_files:
            try:
                eval_data = evaluate_model_results(str(csv_file), evaluator)
                evaluations.append(eval_data)
            except Exception as e:
                print(f"Error evaluating {csv_file}: {e}")
        
        if not evaluations:
            print("Error: No evaluations completed successfully")
            return
        
        # Save individual evaluation results (one CSV per model)
        print("\nSaving individual evaluation results...")
        save_individual_evaluation_csv(evaluations, output_dir)
        
        # Save complete evaluation results (combined CSV for visualization)
        print("Saving complete evaluation results...")
        save_complete_evaluation_csv(evaluations, output_dir)
    
    # Generate visualizations
    print("\nGenerating visualizations...")
    create_comparison_visualizations(evaluations, output_dir)
    
    # Generate detailed report
    print("Generating detailed report...")
    generate_detailed_report(evaluations, output_dir)
    
    # Print summary
    print("\n" + "="*80)
    print("EVALUATION COMPLETE")
    print("="*80)
    print(f"Results saved to: {output_dir}")
    print("\nModel Rankings:")
    sorted_evals = sorted(evaluations, key=lambda x: x['summary']['mean_overall_score'], reverse=True)
    for rank, eval_data in enumerate(sorted_evals, 1):
        score = eval_data['summary']['mean_overall_score']
        print(f"  {rank}. {eval_data['model_name']:30s} - {score:.4f}")


if __name__ == "__main__":
    main()

